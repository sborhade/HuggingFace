{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['OPENAI_API_KEY']=os.getenv(\"OPENAI_API_KEY\")\n",
    "## Langsmith Tracking\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]=os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\"\n",
    "huggingfacehub_api_token=os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "os.environ[\"LANGCHAIN_PROJECT\"]=os.getenv(\"LANGCHAIN_PROJECT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "## Data Ingestion--From the website we need to scrape the data\n",
    "from langchain_community.document_loaders import WebBaseLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader=WebBaseLoader(\"https://huggingface.co/blog/personal-copilot\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://huggingface.co/blog/personal-copilot', 'title': 'Personal Copilot: Train Your Own Coding Assistant', 'description': 'Weâ€™re on a journey to advance and democratize artificial intelligence through open source and open science.', 'language': 'No language found.'}, page_content='\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\nPersonal Copilot: Train Your Own Coding Assistant\\n\\n\\n\\n\\n\\n\\n\\n\\nHugging Face\\n\\n\\n\\n\\n\\n\\n\\n\\t\\t\\t\\t\\tModels\\n\\n\\t\\t\\t\\t\\tDatasets\\n\\n\\t\\t\\t\\t\\tSpaces\\n\\n\\t\\t\\t\\t\\tPosts\\n\\n\\t\\t\\t\\t\\tDocs\\n\\n\\t\\t\\t\\t\\tEnterprise\\n\\nPricing\\n\\t\\t\\t\\n\\n\\n\\n\\n\\n\\nLog In\\n\\t\\t\\t\\t\\nSign Up\\n\\t\\t\\t\\t\\t\\n\\n\\n\\n\\t\\t\\t\\t\\t\\tBack to Articles\\n\\n\\n\\n\\n\\n\\t\\tPersonal Copilot: Train Your Own Coding Assistant\\n\\t\\n\\n\\nPublished\\n\\t\\t\\t\\tOctober 27, 2023\\nUpdate on GitHub\\n\\n\\n\\t\\tUpvote\\n\\n\\t\\t35\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n+29\\n\\n\\n\\nsmangrul\\nSourab Mangrulkar\\n\\n\\n\\n\\n\\nsayakpaul\\nSayak Paul\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nData Collection Workflow\\n\\nFinetuning your own Personal Co-Pilot\\n\\nFull Finetuning\\n\\nPEFT\\n\\nComparison\\n\\nHow do I use it in VS Code?\\nSetting an Inference Endpoint\\n\\nSetting up the VS Code Extension\\n\\n\\nFinetuning your own Code Chat Assistant\\n\\nDance of LoRAs\\nMix-and-Match LoRAs\\n\\nTransfer LoRAs to different base models\\n\\n\\nHow do I run it locally?\\nConclusion\\n\\nAcknowledgements\\n\\n\\nIn the ever-evolving landscape of programming and software development, the quest for efficiency and productivity has led to remarkable innovations. One such innovation is the emergence of code generation models such as Codex, StarCoder and Code Llama. These models have demonstrated remarkable capabilities in generating human-like code snippets, thereby showing immense potential as coding assistants.\\nHowever, while these pre-trained models can perform impressively across a range of tasks, there\\'s an exciting possibility lying just beyond the horizon: the ability to tailor a code generation model to your specific needs. Think of personalized coding assistants which could be leveraged at an enterprise scale. \\nIn this blog post we show how we created HugCoder ðŸ¤—, a code LLM fine-tuned on the code contents from the public repositories of the huggingface GitHub organization. We will discuss our data collection workflow, our training experiments, and some interesting results. This will enable you to create your own personal copilot based on your proprietary codebase. We will leave you with a couple of further extensions of this project for experimentation. \\nLetâ€™s begin ðŸš€\\n\\n\\n\\n\\n\\n\\n\\t\\tData Collection Workflow\\n\\t\\n\\nOur desired dataset is conceptually simple, we structured it like so:\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nRepository Name\\nFilepath in the Repository\\nFile Contents\\n\\n\\n---\\n---\\n---\\n\\n\\n---\\n---\\n---\\n\\n\\n\\n\\nScraping code contents from GitHub is straightforward with the Python GitHub API. However, depending on the number of repositories and the number of code files within a repository, one might easily run into API rate-limiting issues. \\nTo prevent such problems, we decided to clone all the public repositories locally and extract the contents from them instead of through the API. We used the multiprocessing module from Python to download all repos in parallel, as shown in this download script. \\nA repository can often contain non-code files such as images, presentations and other assets. Weâ€™re not interested in scraping them. We created a list of extensions to filter them out. To parse code files other than Jupyter Notebooks, we simply used the \"utf-8\" encoding. For notebooks, we only considered the code cells.\\nWe also excluded all file paths that were not directly related to code. These include: .git, __pycache__, and xcodeproj. \\nTo keep the serialization of this content relatively memory-friendly, we used chunking and the feather format. Refer to this script for the full implementation. \\nThe final dataset is available on the Hub, and it looks like this:\\n\\nFor this blog, we considered the top 10 Hugging Face public repositories, based on stargazers. They are the following: \\n\\n[\\'transformers\\', \\'pytorch-image-models\\', \\'datasets\\', \\'diffusers\\', \\'peft\\', \\'tokenizers\\', \\'accelerate\\', \\'text-generation-inference\\', \\'chat-ui\\', \\'deep-rl-class\\']\\n\\nThis is the code we used to generate this dataset, and this is the dataset in the Hub. Here is a snapshot of what it looks like: \\n\\nTo reduce the project complexity, we didnâ€™t consider deduplication of the dataset. If you are interested in applying deduplication techniques for a production application, this blog post is an excellent resource about the topic in the context of code LLMs.\\n\\n\\n\\n\\n\\n\\t\\tFinetuning your own Personal Co-Pilot\\n\\t\\n\\nIn this section, we show how to fine-tune the following models: bigcode/starcoder (15.5B params), bigcode/starcoderbase-1b (1B params), Deci/DeciCoder-1b (1B params). We\\'ll use a single A100 40GB Colab Notebook using ðŸ¤— PEFT (Parameter-Efficient Fine-Tuning) for all the experiments. Additionally, we\\'ll show how to fully finetune the bigcode/starcoder (15.5B params) on a machine with 8 A100 80GB GPUs using ðŸ¤— Accelerate\\'s FSDP integration. The training objective is fill in the middle (FIM), wherein parts of a training sequence are moved to the end, and the reordered sequence is predicted auto-regressively.\\nWhy PEFT? Full fine-tuning is expensive. Letâ€™s have some numbers to put things in perspective:\\nMinimum GPU memory required for full fine-tuning:\\n\\nWeight: 2 bytes (Mixed-Precision training)\\nWeight gradient: 2 bytes\\nOptimizer state when using Adam: 4 bytes for original FP32 weight + 8 bytes for first and second moment estimates\\nCost per parameter adding all of the above: 16 bytes per parameter \\n15.5B model -> 248GB of GPU memory without even considering huge memory requirements for storing intermediate activations -> minimum 4X A100 80GB GPUs required\\n\\nSince the hardware requirements are huge, we\\'ll be using parameter-efficient fine-tuning using QLoRA. Here are the minimal GPU memory requirements for fine-tuning StarCoder using QLoRA:\\n\\ntrainable params: 110,428,160 || all params: 15,627,884,544 || trainable%: 0.7066097761926236\\n\\n\\nBase model Weight: 0.5 bytes * 15.51B frozen params  = 7.755 GB\\nAdapter weight: 2 bytes * 0.11B trainable params        = 0.22GB\\nWeight gradient: 2 bytes * 0.11B trainable params       = 0.12GB\\nOptimizer state when using Adam:  4 bytes * 0.11B trainable params * 3 = 1.32GB\\nAdding all of the above -> 9.51 GB ~10GB -> 1 A100 40GB GPU required ðŸ¤¯. The reason for A100 40GB GPU is that the intermediate activations for long sequence lengths of 2048 and batch size of 4 for training lead to higher memory requirements. As we will see below, GPU memory required is 26GB which can be accommodated on A100 40GB GPU. Also, A100 GPUs have better compatibilty with Flash Attention 2.\\n\\nIn the above calculations, we didn\\'t consider memory required for intermediate activation checkpointing which is considerably huge. We leverage Flash Attention V2 and Gradient Checkpointing to overcome this issue. \\n\\nFor QLoRA along with flash attention V2 and gradient checkpointing, the total memory occupied by the model on a single A100 40GB GPU is 26 GB with a batch size of 4.\\nFor full fine-tuning using FSDP along with Flash Attention V2 and Gradient Checkpointing, the memory occupied per GPU ranges between 70 GB to 77.6 GB with a per_gpu_batch_size of 1.\\n\\nPlease refer to the model-memory-usage to easily calculate how much vRAM is needed to train and perform big model inference on a model hosted on the ðŸ¤— Hugging Face Hub.\\n\\n\\n\\n\\n\\n\\t\\tFull Finetuning\\n\\t\\n\\nWe will look at how to do full fine-tuning of bigcode/starcoder (15B params) on 8 A100 80GB GPUs using PyTorch Fully Sharded Data Parallel (FSDP) technique. For more information on FSDP, please refer to Fine-tuning Llama 2 70B using PyTorch FSDP and Accelerate Large Model Training using PyTorch Fully Sharded Data Parallel.\\nResources\\n\\nCodebase: link. It uses the recently added Flash Attention V2 support in Transformers. \\nFSDP Config: fsdp_config.yaml\\nModel: bigcode/stacoder\\nDataset: smangrul/hf-stack-v1\\nFine-tuned Model: smangrul/peft-lora-starcoder15B-v2-personal-copilot-A100-40GB-colab\\n\\nThe command to launch training is given at run_fsdp.sh.\\naccelerate launch --config_file \"configs/fsdp_config.yaml\"  train.py \\\\\\n    --model_path \"bigcode/starcoder\" \\\\\\n    --dataset_name \"smangrul/hf-stack-v1\" \\\\\\n    --subset \"data\" \\\\\\n    --data_column \"content\" \\\\\\n    --split \"train\" \\\\\\n    --seq_length 2048 \\\\\\n    --max_steps 2000 \\\\\\n    --batch_size 1 \\\\\\n    --gradient_accumulation_steps 2 \\\\\\n    --learning_rate 5e-5 \\\\\\n    --lr_scheduler_type \"cosine\" \\\\\\n    --weight_decay 0.01 \\\\\\n    --num_warmup_steps 30 \\\\\\n    --eval_freq 100 \\\\\\n    --save_freq 500 \\\\\\n    --log_freq 25 \\\\\\n    --num_workers 4 \\\\\\n    --bf16 \\\\\\n    --no_fp16 \\\\\\n    --output_dir \"starcoder-personal-copilot-A100-40GB-colab\" \\\\\\n    --fim_rate 0.5 \\\\\\n    --fim_spm_rate 0.5 \\\\\\n    --use_flash_attn\\n\\nThe total training time was 9 Hours. Taking the cost of $12.00 / hr based on lambdalabs for 8x A100 80GB GPUs, the total cost would be $108.\\n\\n\\n\\n\\n\\n\\t\\tPEFT\\n\\t\\n\\nWe will look at how to use QLoRA for fine-tuning bigcode/starcoder (15B params) on a single A100 40GB GPU using ðŸ¤— PEFT. For more information on QLoRA and PEFT methods, please refer to Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA and ðŸ¤— PEFT: Parameter-Efficient Fine-Tuning of Billion-Scale Models on Low-Resource Hardware.\\nResources\\n\\nCodebase: link. It uses the recently added Flash Attention V2 support in Transformers. \\nColab notebook: link. Make sure to choose A100 GPU with High RAM setting.\\nModel: bigcode/stacoder\\nDataset: smangrul/hf-stack-v1\\nQLoRA Fine-tuned Model: smangrul/peft-lora-starcoder15B-v2-personal-copilot-A100-40GB-colab\\n\\nThe command to launch training is given at run_peft.sh. The total training time was 12.5 Hours. Taking the cost of $1.10 / hr based on lambdalabs, the total cost would be $13.75. That\\'s pretty good ðŸš€! In terms of cost, it\\'s 7.8X lower than the cost for full fine-tuning.  \\n\\n\\n\\n\\n\\n\\t\\tComparison\\n\\t\\n\\nThe plot below shows the eval loss, train loss and learning rate scheduler for QLoRA vs full fine-tuning. We observe that full fine-tuning leads to slightly lower loss and converges a bit faster compared to QLoRA. The learning rate for peft fine-tuning is 10X more than that of full fine-tuning.\\n\\nTo make sure that our QLoRA model doesn\\'t lead to catastrophic forgetting, we run the Python Human Eval on it. Below are the results we got. Pass@1 measures the pass rate of completions considering just a single generated code candidate per problem. We can observe that the performance on humaneval-python is comparable between the base bigcode/starcoder (15B params) and the fine-tuned PEFT model smangrul/peft-lora-starcoder15B-v2-personal-copilot-A100-40GB-colab.\\n\\n\\n\\n\\n\\n\\n\\n\\nModel\\nPass@1\\n\\n\\nbigcode/starcoder\\n33.57\\n\\n\\nsmangrul/peft-lora-starcoder15B-v2-personal-copilot-A100-40GB-colab\\n33.37\\n\\n\\n\\n\\nLet\\'s now look at some qualitative samples. In our manual analysis, we noticed that the QLoRA led to slight overfitting and as such we down weigh it by creating new weighted adapter with weight 0.8 via add_weighted_adapter utility of PEFT.\\nWe will look at 2 code infilling examples wherein the task of the model is to fill the part denoted by the <FILL_ME> placeholder. We will consider infilling completions from GitHub Copilot, the QLoRA fine-tuned model and the full fine-tuned model. \\n\\nQualitative Example 1\\nIn the example above, the completion from GitHub Copilot is along the correct lines but doesn\\'t help much. On the other hand, completions from QLoRA and full fine-tuned models are correctly infilling the entire function call with the necessary parameters. However, they are also adding a lot more noise afterwards. This could be controlled with a post-processing step to limit completions to closing brackets or new lines. Note that both QLoRA and full fine-tuned models produce results with similar quality.\\n\\nQualitative Example 2\\nIn the second example above, GitHub Copilot didn\\'t give any completion. This can be due to the fact that ðŸ¤— PEFT is a recent library and not yet part of Copilot\\'s training data, which is exactly the type of problem we are trying to address. On the other hand, completions from QLoRA and full fine-tuned models are correctly infilling the entire function call with the necessary parameters. Again, note that both the QLoRA and the full fine-tuned models are giving generations of similar quality. Inference Code with various examples for full fine-tuned model and peft model are available at Full_Finetuned_StarCoder_Inference.ipynb and PEFT_StarCoder_Inference.ipynb, respectively.\\nTherefore, we can observe that the generations from both the variants are as per expectations. Awesome! ðŸš€\\n\\n\\n\\n\\n\\n\\t\\tHow do I use it in VS Code?\\n\\t\\n\\nYou can easily configure a custom code-completion LLM in VS Code using ðŸ¤— llm-vscode VS Code Extension, together with hosting the model via ðŸ¤— Inference EndPoints. We\\'ll go through the required steps below. You can learn more details about deploying an endpoint in the inference endpoints documentation.\\n\\n\\n\\n\\n\\n\\t\\tSetting an Inference Endpoint\\n\\t\\n\\nBelow are the screenshots with the steps we followed to create our custom Inference Endpoint. We used our QLoRA model, exported as a full-sized merged model that can be easily loaded in transformers.\\n\\n\\n\\n\\n\\n\\n\\n\\t\\tSetting up the VS Code Extension\\n\\t\\n\\nJust follow the installation steps. In the settings, replace the endpoint in the field below, so it points to the HF Inference Endpoint you deployed.\\n\\nUsage will look like below:\\n\\n\\n\\n\\n\\n\\n\\t\\tFinetuning your own Code Chat Assistant\\n\\t\\n\\nSo far, the models we trained were specifically trained as personal co-pilot for code completion tasks. They aren\\'t trained to carry out conversations or for question answering. Octocoder and StarChat are great examples of such models. This section briefly describes how to achieve that.\\nResources\\n\\nCodebase: link. It uses the recently added Flash Attention V2 support in Transformers. \\nColab notebook: link. Make sure to choose A100 GPU with High RAM setting.\\nModel: bigcode/stacoderplus\\nDataset: smangrul/code-chat-assistant-v1. Mix of LIMA+GUANACO with proper formatting in a ready-to-train format.\\nTrained Model: smangrul/peft-lora-starcoderplus-chat-asst-A100-40GB-colab\\n\\n\\n\\n\\n\\n\\n\\t\\tDance of LoRAs\\n\\t\\n\\nIf you have dabbled with Stable Diffusion models and LoRAs for making your own Dreambooth models, you might be familiar with the concepts of combining different LoRAs with different weights, using a LoRA model with a different base model than the one on which it was trained. In text/code domain, this remains unexplored territory. We carry out experiments in this regard and have observed very promising findings. Are you ready? Let\\'s go! ðŸš€\\n\\n\\n\\n\\n\\n\\t\\tMix-and-Match LoRAs\\n\\t\\n\\nPEFT currently supports 3 ways of combining LoRA models, linear, svd and cat. For more details, refer to tuners#peft.LoraModel.add_weighted_adapter.\\nOur notebook Dance_of_LoRAs.ipynb includes all the inference code and various LoRA loading combinations, like loading the chat assistant on top of starcoder instead of starcodeplus, which is the base model that we fine-tuned. \\nHere, we will consider 2 abilities (chatting/QA and code-completion) on 2 data distributions (top 10 public hf codebase and generic codebase). That gives us 4 axes on which we\\'ll carry out some qualitative evaluation analyses.\\n\\n\\n\\n\\n\\n\\t\\tFirst, let us consider the chatting/QA task.\\n\\t\\n\\nIf we disable adapters, we observe that the task fails for both datasets, as the base model (starcoder) is only meant for code completion and not suitable for chatting/question-answering. Enabling copilot adapter performs similar to the disabled case because this LoRA was also specifically fine-tuned for code-completion.\\nNow, let\\'s enable the assistant adapter.\\n\\nQuestion Answering based on generic code\\n\\nQuestion Answering based on HF code\\nWe can observe that generic question regarding scrapy is being answered properly. However, it is failing for the HF code related question which wasn\\'t part of its pretraining data.\\n\\n\\n\\n\\n\\n\\t\\tLet us now consider the code-completion task.\\n\\t\\n\\nOn disabling adapters, we observe that the code completion for the generic two-sum works as expected. However, the HF code completion fails with wrong params to LoraConfig, because the base model hasn\\'t seen it in its pretraining data. Enabling assistant performs similar to the disabled case as it was trained on natural language conversations which didn\\'t have any Hugging Face code repos.\\nNow, let\\'s enable the copilot adapter.\\n\\nWe can observe that the copilot adapter gets it right in both cases. Therefore, it performs as expected for code-completions when working with HF specific codebase as well as generic codebases.\\nNow, as a user, I want to combine the ability of assistant as well as copilot. This will enable me to use it for code completion while coding in an IDE, and also have it as a chatbot to answer my questions regarding APIs, classes, methods, documentation. It should be able to provide answers to questions like How do I use x, Please write a code snippet for Y on my codebase.\\nPEFT allows you to do it via add_weighted_adapter. Let\\'s create a new adapter code_buddy with equal weights to assistant and copilot adapters.\\n\\nCombining Multiple Adapters\\nNow, let\\'s see how code_buddy performs on the chatting/question_answering tasks.\\n\\nWe can observe that code_buddy is performing much better than the assistant or copilot adapters alone! It is able to answer the write a code snippet request to show how to use a specific HF repo API. However, it is also hallucinating the wrong links/explanations, which remains an open challenge for LLMs.\\nBelow is the performance of code_buddy on code completion tasks.\\n\\nWe can observe that code_buddy is performing on par with copilot, which was specifically finetuned for this task.\\n\\n\\n\\n\\n\\n\\t\\tTransfer LoRAs to different base models\\n\\t\\n\\nWe can also transfer the LoRA models to different base models.\\nWe will take the hot-off-the-press Octocoder model and apply on it the LoRA we trained above with starcoder base model. Please go through the following notebook PEFT_Personal_Code_CoPilot_Adapter_Transfer_Octocoder.ipynb for the entire code.\\nPerformance on the Code Completion task\\n\\nWe can observe that octocoder is performing great. It is able to complete HF specific code snippets. It is also able to complete generic code snippets as seen in the notebook.\\nPerformance on the Chatting/QA task\\nAs Octocoder is trained to answer questions and carry out conversations about coding, let\\'s see if it can use our LoRA adapter to answer HF specific questions. \\n\\nYay! It correctly answers in detail how to create LoraConfig and related peft model along with correctly using the model name, dataset name as well as param values of LoraConfig. On disabling the adapter, it fails to correctly use the API of LoraConfig or to create a PEFT model, suggesting that it isn\\'t part of the training data of Octocoder.\\n\\n\\n\\n\\n\\n\\t\\tHow do I run it locally?\\n\\t\\n\\nI know, after all this, you want to finetune starcoder on your codebase and use it locally on your consumer hardware such as Mac laptops with M1 GPUs, windows with RTX 4090/3090 GPUs ... \\nDon\\'t worry, we have got you covered.\\nWe will be using this super cool open source library mlc-llm ðŸ”¥. Specifically, we will be using this fork pacman100/mlc-llm which has changes to get it working with the Hugging Face Code Completion extension for VS Code. On my Mac latop with M1 Metal GPU, the 15B model was painfully slow. Hence, we will go small and train a PEFT LoRA version as well as a full finetuned version of bigcode/starcoderbase-1b. The training colab notebooks are linked below:\\n\\nColab notebook for Full fine-tuning and PEFT LoRA finetuning of starcoderbase-1b: link\\n\\nThe training loss, evaluation loss as well as learning rate schedules are plotted below:\\n\\nNow, we will look at detailed steps for locally hosting the merged model smangrul/starcoder1B-v2-personal-copilot-merged and using it with ðŸ¤— llm-vscode VS Code Extension. \\n\\nClone the repo\\n\\ngit clone --recursive https://github.com/pacman100/mlc-llm.git && cd mlc-llm/\\n\\n\\nInstall the mlc-ai and mlc-chat (in editable mode) :\\n\\npip install --pre --force-reinstall mlc-ai-nightly mlc-chat-nightly -f https://mlc.ai/wheels\\ncd python\\npip uninstall mlc-chat-nightly\\npip install -e \".\"\\n\\n\\nCompile the model via:\\n\\ntime python3 -m mlc_llm.build --hf-path smangrul/starcoder1B-v2-personal-copilot-merged --target metal  --use-cache=0\\n\\n\\nUpdate the config with the following values in dist/starcoder1B-v2-personal-copilot-merged-q4f16_1/params/mlc-chat-config.json:\\n\\n{\\n    \"model_lib\": \"starcoder7B-personal-copilot-merged-q4f16_1\",\\n    \"local_id\": \"starcoder7B-personal-copilot-merged-q4f16_1\",\\n    \"conv_template\": \"code_gpt\",\\n-    \"temperature\": 0.7,\\n+    \"temperature\": 0.2,\\n-    \"repetition_penalty\": 1.0,\\n    \"top_p\": 0.95,\\n-    \"mean_gen_len\": 128,\\n+    \"mean_gen_len\": 64,\\n-    \"max_gen_len\": 512,\\n+    \"max_gen_len\": 64, \\n    \"shift_fill_factor\": 0.3,\\n    \"tokenizer_files\": [\\n        \"tokenizer.json\",\\n        \"merges.txt\",\\n        \"vocab.json\"\\n    ],\\n    \"model_category\": \"gpt_bigcode\",\\n    \"model_name\": \"starcoder1B-v2-personal-copilot-merged\"\\n}\\n\\n\\nRun the local server:\\n\\n python -m mlc_chat.rest --model dist/starcoder1B-v2-personal-copilot-merged-q4f16_1/params --lib-path dist/starcoder1B-v2-personal-copilot-merged-q4f16_1/starcoder1B-v2-personal-copilot-merged-q4f16_1-metal.so\\n\\n\\nChange the endpoint of HF Code Completion extension in VS Code to point to the local server:\\n\\n\\n\\nOpen a new file in VS code, paste the code below and have the cursor in-between the doc quotes, so that the model tries to infill the doc string:\\n\\n\\nVoila! â\\xad�ï¸�\\nThe demo at the start of this post is this 1B model running locally on my Mac laptop.\\n\\n\\n\\n\\n\\n\\t\\tConclusion\\n\\t\\n\\nIn this blog plost, we saw how to finetune starcoder to create a personal co-pilot that knows about our code. We called it ðŸ¤— HugCoder, as we trained it on Hugging Face code :) After looking at the data collection workflow, we compared training using QLoRA vs full fine-tuning. We also experimented by combining different LoRAs, which is still an unexplored technique in the text/code domain. For deployment, we examined remote inference using ðŸ¤— Inference Endpoints, and also showed on-device execution of a smaller model with VS Code and MLC.\\nPlease, let us know if you use these methods for your own codebase!\\n\\n\\n\\n\\n\\n\\t\\tAcknowledgements\\n\\t\\n\\nWe would like to thank Pedro Cuenca, Leandro von Werra, Benjamin Bossan, Sylvain Gugger and Loubna Ben Allal for their help with the writing of this blogpost.\\n\\nMore Articles from our Blog\\n\\n\\nSafeCoder vs. Closed-source Code Assistants\\n\\n\\t\\t\\t\\tByÂ\\xa0\\njuliensimon\\n\\n\\nSeptember 11, 2023\\n\\n\\nIntroducing SafeCoder\\n\\n\\t\\t\\t\\tByÂ\\xa0\\njeffboudier\\n\\n\\nAugust 22, 2023\\n\\n\\n\\n\\t\\tUpvote\\n\\n\\t\\t35\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n+23\\n\\n\\n\\n\\n\\n\\t\\t\\tSystem theme\\n\\t\\t\\n\\nCompany\\nTOS\\nPrivacy\\nAbout\\nJobs\\n\\nWebsite\\nModels\\nDatasets\\nSpaces\\nPricing\\nDocs\\n\\n\\n\\n\\n\\n')]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs=loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load Data--> Docs-->Divide our Docuemnts into chunks dcouments-->text-->vectors-->Vector Embeddings--->Vector Store DB\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200)\n",
    "documents=text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sande\\AppData\\Local\\Temp\\ipykernel_16908\\1035242831.py:2: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  model_for_embedding = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "model_for_embedding = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "#convert to string before embedding:\n",
    "#documents=[str(doc) for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstoredb=FAISS.from_documents(documents,model_for_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x27f62bdd520>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstoredb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We also excluded all file paths that were not directly related to code. These include: .git, __pycache__, and xcodeproj. \\nTo keep the serialization of this content relatively memory-friendly, we used chunking and the feather format. Refer to this script for the full implementation. \\nThe final dataset is available on the Hub, and it looks like this:'"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"xcodeproj\"\n",
    "response = vectorstoredb.similarity_search(query)\n",
    "response[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"HuggingFaceH4/zephyr-7b-beta\",\n",
    "    task=\"text-generation\",\n",
    "    max_new_tokens=512,\n",
    "    do_sample=False,\n",
    "    repetition_penalty=1.03,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableLambda(format_docs)\n",
       "}), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "| ChatPromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\nAnswer the following question based only on the provided context:\\n<context>\\n{context}\\n</context>\\n\\n\\n'), additional_kwargs={})])\n",
       "| HuggingFaceEndpoint(repo_id='HuggingFaceH4/zephyr-7b-beta', huggingfacehub_api_token='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', repetition_penalty=1.03, stop_sequences=[], server_kwargs={}, model_kwargs={}, model='HuggingFaceH4/zephyr-7b-beta', client=<InferenceClient(model='HuggingFaceH4/zephyr-7b-beta', timeout=120)>, async_client=<InferenceClient(model='HuggingFaceH4/zephyr-7b-beta', timeout=120)>, task='text-generation')\n",
       "| StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Retrieval Chain, Document chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt=ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "Answer the following question based only on the provided context:\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "document_chain=create_stuff_documents_chain(llm,prompt)\n",
    "document_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Question:\\nWhat are \"total traces\" and \"extended traces\" in the context of LangSmith?'"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "document_chain.invoke({\n",
    "    \"input\":\"LangSmith has two usage limits: total traces and extended\",\n",
    "    \"context\":[Document(page_content=\"LangSmith has two usage limits: total traces and extended traces. These correspond to the two metrics we've been tracking on our usage graph. \")]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever=vectorstoredb.as_retriever()\n",
    "from langchain.chains import create_retrieval_chain\n",
    "retrieval_chain=create_retrieval_chain(retriever,document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableBinding(bound=RunnableLambda(lambda x: x['input'])\n",
       "           | VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x0000027F62BDD520>, search_kwargs={}), kwargs={}, config={'run_name': 'retrieve_documents'}, config_factories=[])\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "              context: RunnableLambda(format_docs)\n",
       "            }), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "            | ChatPromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\nAnswer the following question based only on the provided context:\\n<context>\\n{context}\\n</context>\\n\\n\\n'), additional_kwargs={})])\n",
       "            | HuggingFaceEndpoint(repo_id='HuggingFaceH4/zephyr-7b-beta', huggingfacehub_api_token='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', repetition_penalty=1.03, stop_sequences=[], server_kwargs={}, model_kwargs={}, model='HuggingFaceH4/zephyr-7b-beta', client=<InferenceClient(model='HuggingFaceH4/zephyr-7b-beta', timeout=120)>, async_client=<InferenceClient(model='HuggingFaceH4/zephyr-7b-beta', timeout=120)>, task='text-generation')\n",
       "            | StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])\n",
       "  }), kwargs={}, config={'run_name': 'retrieval_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sure! Machine learning is a subfield of artificial intelligence that focuses on enabling computer systems to automatically learn and improve from experience without being explicitly programmed. The basic idea is to feed large amounts of data and let the machine learn patterns and relationships within the data, allowing it to make predictions or decisions based on new, unseen data.\\n\\nMachine learning algorithms use various techniques such as supervised learning, unsupervised learning, reinforcement learning, and deep learning to learn from data. \\n'"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"HuggingFaceH4/zephyr-7b-beta\",\n",
    "    task=\"text-generation\",\n",
    "    max_new_tokens=512,\n",
    "    do_sample=False,\n",
    "    repetition_penalty=1.03,\n",
    ")\n",
    "\n",
    "chat_model = ChatHuggingFace(llm=llm)\n",
    "# Without bind.\n",
    "chain = (\n",
    "    chat_model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain.invoke(\"Repeat quoted words exactly: 'One two three four five.'\")\n",
    "# Output is 'One two three four five.'\n",
    "\n",
    "# With bind.\n",
    "chain = (\n",
    "    chat_model.bind(stop=[\"three\"])\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain.invoke(\"Repeat quoted words exactly: 'One two three four five.'\")\n",
    "# Output is 'One two'\n",
    "\n",
    "from langchain_core.messages import (\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"Machine learning\"),\n",
    "    HumanMessage(\n",
    "        content=\"let me know about machine learning?\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "ai_msg = chat_model.invoke(messages)\n",
    "ai_msg.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nQuestion: Can you provide a summary of the results from the Python Human Eval on the QLoRA model compared to the fine-tuned PEFT model?'"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Get the response form the LLM\n",
    "response=retrieval_chain.invoke({\"input\":\"LangSmith has two usage limits: total traces and extended\"})\n",
    "response['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
